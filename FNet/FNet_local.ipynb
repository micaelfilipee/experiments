{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNet_local.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSf22fxpauGC",
        "outputId": "c643a376-6939-41fb-9f6e-58becd613679"
      },
      "source": [
        "!pip3 install labml"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: labml in ./.local/lib/python3.9/site-packages (0.4.121)\r\n",
            "Requirement already satisfied: gitpython in ./.local/lib/python3.9/site-packages (from labml) (3.1.14)\r\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from labml) (1.20.3)\r\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from labml) (5.3.1)\r\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.local/lib/python3.9/site-packages (from gitpython->labml) (4.0.7)\r\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in ./.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython->labml) (4.0.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khabt09ybYPz",
        "outputId": "8aefaa6f-f89a-4328-9e5f-1fad3bfd9f52"
      },
      "source": [
        "!pip3 install labml-helpers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: labml-helpers in ./.local/lib/python3.9/site-packages (0.4.77)\r\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (from labml-helpers) (1.9.0)\r\n",
            "Requirement already satisfied: labml>=0.4.97 in ./.local/lib/python3.9/site-packages (from labml-helpers) (0.4.121)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from labml>=0.4.97->labml-helpers) (5.3.1)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from labml>=0.4.97->labml-helpers) (1.20.3)\n",
            "Requirement already satisfied: gitpython in ./.local/lib/python3.9/site-packages (from labml>=0.4.97->labml-helpers) (3.1.14)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.local/lib/python3.9/site-packages (from gitpython->labml>=0.4.97->labml-helpers) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in ./.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.97->labml-helpers) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch->labml-helpers) (3.10.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_-_Q8elbmPr",
        "outputId": "c0d36527-b2c7-405f-ba15-6d20e8997b01"
      },
      "source": [
        "!pip3 install labml-nn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting labml-nn\n",
            "  Downloading labml_nn-0.4.100-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: labml>=0.4.110 in ./.local/lib/python3.9/site-packages (from labml-nn) (0.4.121)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from labml-nn) (1.20.3)\n",
            "Requirement already satisfied: labml-helpers>=0.4.77 in ./.local/lib/python3.9/site-packages (from labml-nn) (0.4.77)\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (from labml-nn) (1.9.0)\n",
            "Requirement already satisfied: gitpython in ./.local/lib/python3.9/site-packages (from labml>=0.4.110->labml-nn) (3.1.14)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from labml>=0.4.110->labml-nn) (5.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.local/lib/python3.9/site-packages (from gitpython->labml>=0.4.110->labml-nn) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in ./.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.110->labml-nn) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch->labml-nn) (3.10.0.0)\n",
            "Installing collected packages: einops, labml-nn\n",
            "Successfully installed einops-0.3.0 labml-nn-0.4.100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy1U1R1iMKge",
        "outputId": "df84e30d-3316-4f48-ab49-e216b47bb591"
      },
      "source": [
        "!pip3 install torch"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (1.9.0)\r\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch) (3.10.0.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caVFLPO11yCc",
        "outputId": "07dcb9a8-d001-4046-e680-0def67ec26fd"
      },
      "source": [
        "#RTX3060\n",
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to create the collection: Prompt dismissed..\u001b[0m\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp39-cp39-linux_x86_64.whl (2041.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2041.4 MB 23 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp39-cp39-linux_x86_64.whl (23.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp39-cp39-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch==1.9.0+cu111) (3.10.0.0)\n",
            "Collecting pillow>=5.3.0\n",
            "  Downloading Pillow-8.2.0-cp39-cp39-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 27.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from torchvision==0.10.0+cu111) (1.20.3)\n",
            "Installing collected packages: torch, pillow, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0\n",
            "    Uninstalling torch-1.9.0:\n",
            "      Successfully uninstalled torch-1.9.0\n",
            "Successfully installed pillow-8.2.0 torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDJjvRHJeJcM",
        "outputId": "60c5d7f2-5d87-4e1c-967c-e0d9394f777e"
      },
      "source": [
        "!pip3 install torchtext"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.10.0-cp39-cp39-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 2.9 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.61.1-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchtext) (2.25.1)\n",
            "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from torchtext) (1.20.3)\n",
            "Requirement already satisfied: torch==1.9.0 in ./.local/lib/python3.9/site-packages (from torchtext) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in ./.local/lib/python3.9/site-packages (from torch==1.9.0->torchtext) (3.10.0.0)\n",
            "Installing collected packages: tqdm, torchtext\n",
            "Successfully installed torchtext-0.10.0 tqdm-4.61.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEvpIdjoReIv"
      },
      "source": [
        "#apenas para execução local\n",
        "#!pip3 install --upgrade git+https://github.com/pytorch/text\n",
        "#import sys\n",
        "#sys.path.append(\"./.local/lib/python3.9/site-packages\")\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EnD4rF4FyEA"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from labml import experiment\n",
        "from labml.configs import option\n",
        "from labml_helpers.module import Module\n",
        "from labml_nn.experiments.nlp_classification import NLPClassificationConfigs\n",
        "from labml_nn.transformers import Encoder\n",
        "from labml_nn.transformers import TransformerConfigs\n",
        "from labml_nn.transformers.utils import subsequent_mask\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoj8BKZF5ckn"
      },
      "source": [
        "from collections import Counter\n",
        "from typing import Callable\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "from labml import lab, tracker, monit\n",
        "from labml.configs import option\n",
        "from labml_helpers.device import DeviceConfigs\n",
        "from labml_helpers.metrics.accuracy import Accuracy\n",
        "from labml_helpers.module import Module\n",
        "from labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\n",
        "from labml_nn.optimizers.configs import OptimizerConfigs\n",
        "\n",
        "class NLPClassificationConfigs(TrainValidConfigs):\n",
        "    \"\"\"\n",
        "    <a id=\"NLPClassificationConfigs\">\n",
        "    ## Trainer configurations\n",
        "    </a>\n",
        "    This has the basic configurations for NLP classification task training.\n",
        "    All the properties are configurable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer: torch.optim.Adam\n",
        "    # Training device\n",
        "    device: torch.device = DeviceConfigs()\n",
        "\n",
        "    # Autoregressive model\n",
        "    model: Module\n",
        "    # Batch size\n",
        "    batch_size: int = 16\n",
        "    # Length of the sequence, or context size\n",
        "    seq_len: int = 512\n",
        "    # Vocabulary\n",
        "    vocab: Vocab = 'ag_news'\n",
        "    # Number of token in vocabulary\n",
        "    n_tokens: int\n",
        "    # Number of classes\n",
        "    n_classes: int = 'ag_news'\n",
        "    # Tokenizer\n",
        "    tokenizer: Callable = 'character'\n",
        "\n",
        "    # Whether to periodically save models\n",
        "    is_save_models = True\n",
        "\n",
        "    # Loss function\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    # Accuracy function\n",
        "    accuracy = Accuracy()\n",
        "    # Model embedding size\n",
        "    d_model: int = 512\n",
        "    # Gradient clipping\n",
        "    grad_norm_clip: float = 1.0\n",
        "\n",
        "    # Training data loader\n",
        "    train_loader: DataLoader = 'ag_news'\n",
        "    # Validation data loader\n",
        "    valid_loader: DataLoader = 'ag_news'\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        ### Initialization\n",
        "        \"\"\"\n",
        "        # Set tracker configurations\n",
        "        tracker.set_scalar(\"accuracy.*\", True)\n",
        "        tracker.set_scalar(\"loss.*\", True)\n",
        "        # Add a hook to log module outputs\n",
        "        hook_model_outputs(self.mode, self.model, 'model')\n",
        "        # Add accuracy as a state module.\n",
        "        # The name is probably confusing, since it's meant to store\n",
        "        # states between training and validation for RNNs.\n",
        "        # This will keep the accuracy metric stats separate for training and validation.\n",
        "        self.state_modules = [self.accuracy]\n",
        "\n",
        "    def step(self, batch: any, batch_idx: BatchIndex):\n",
        "        \"\"\"\n",
        "        ### Training or validation step\n",
        "        \"\"\"\n",
        "\n",
        "        # Move data to the device\n",
        "        data, target = batch[0].to(self.device), batch[1].to(self.device)\n",
        "\n",
        "        # Update global step (number of tokens processed) when in training mode\n",
        "        if self.mode.is_train:\n",
        "            tracker.add_global_step(data.shape[1])\n",
        "\n",
        "        # Whether to capture model outputs\n",
        "        with self.mode.update(is_log_activations=batch_idx.is_last):\n",
        "            # Get model outputs.\n",
        "            # It's returning a tuple for states when using RNNs.\n",
        "            # This is not implemented yet. 😜\n",
        "            output, *_ = self.model(data)\n",
        "\n",
        "        # Calculate and log loss\n",
        "        loss = self.loss_func(output, target)\n",
        "        tracker.add(\"loss.\", loss)\n",
        "\n",
        "        # Calculate and log accuracy\n",
        "        self.accuracy(output, target)\n",
        "        self.accuracy.track()\n",
        "\n",
        "        # Train the model\n",
        "        if self.mode.is_train:\n",
        "            # Calculate gradients\n",
        "            loss.backward()\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n",
        "            # Take optimizer step\n",
        "            self.optimizer.step()\n",
        "            # Log the model parameters and gradients on last batch of every epoch\n",
        "            if batch_idx.is_last:\n",
        "                tracker.add('model', self.model)\n",
        "            # Clear the gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        # Save the tracked metrics\n",
        "        tracker.save()\n",
        "\n",
        "\n",
        "@option(NLPClassificationConfigs.optimizer)\n",
        "def _optimizer(c: NLPClassificationConfigs):\n",
        "    \"\"\"\n",
        "    ### Default [optimizer configurations](../optimizers/configs.html)\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = OptimizerConfigs()\n",
        "    optimizer.parameters = c.model.parameters()\n",
        "    optimizer.optimizer = 'Adam'\n",
        "    optimizer.d_model = c.d_model\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "@option(NLPClassificationConfigs.tokenizer)\n",
        "def basic_english():\n",
        "    \"\"\"\n",
        "    ### Basic  english tokenizer\n",
        "    We use character level tokenizer in this experiment.\n",
        "    You can switch by setting,\n",
        "    ```\n",
        "        'tokenizer': 'basic_english',\n",
        "    ```\n",
        "    as the configurations dictionary when starting the experiment.\n",
        "    \"\"\"\n",
        "    from torchtext.data import get_tokenizer\n",
        "    return get_tokenizer('basic_english')\n",
        "\n",
        "\n",
        "def character_tokenizer(x: str):\n",
        "    \"\"\"\n",
        "    ### Character level tokenizer\n",
        "    \"\"\"\n",
        "    return list(x)\n",
        "\n",
        "\n",
        "@option(NLPClassificationConfigs.tokenizer)\n",
        "def character():\n",
        "    \"\"\"\n",
        "    Character level tokenizer configuration\n",
        "    \"\"\"\n",
        "    return character_tokenizer\n",
        "\n",
        "\n",
        "@option(NLPClassificationConfigs.n_tokens)\n",
        "def _n_tokens(c: NLPClassificationConfigs):\n",
        "    \"\"\"\n",
        "    Get number of tokens\n",
        "    \"\"\"\n",
        "    return len(c.vocab) + 2\n",
        "\n",
        "\n",
        "class CollateFunc:\n",
        "    \"\"\"\n",
        "    ## Function to load data into batches\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, vocab: Vocab, seq_len: int, padding_token: int, classifier_token: int):\n",
        "        \"\"\"\n",
        "        * `tokenizer` is the tokenizer function\n",
        "        * `vocab` is the vocabulary\n",
        "        * `seq_len` is the length of the sequence\n",
        "        * `padding_token` is the token used for padding when the `seq_len` is larger than the text length\n",
        "        * `classifier_token` is the `[CLS]` token which we set at end of the input\n",
        "        \"\"\"\n",
        "        self.classifier_token = classifier_token\n",
        "        self.padding_token = padding_token\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"\n",
        "        * `batch` is the batch of data collected by the `DataLoader`\n",
        "        \"\"\"\n",
        "\n",
        "        # Input data tensor, initialized with `padding_token`\n",
        "        data = torch.full((self.seq_len, len(batch)), self.padding_token, dtype=torch.long)\n",
        "        # Empty labels tensor\n",
        "        labels = torch.zeros(len(batch), dtype=torch.long)\n",
        "\n",
        "        # Loop through the samples\n",
        "        for (i, (_label, _text)) in enumerate(batch):\n",
        "            # Set the label\n",
        "            labels[i] = int(_label) - 1\n",
        "            # Tokenize the input text\n",
        "            _text = [self.vocab[token] for token in self.tokenizer(_text)]\n",
        "            # Truncate upto `seq_len`\n",
        "            _text = _text[:self.seq_len]\n",
        "            # Transpose and add to data\n",
        "            data[:len(_text), i] = data.new_tensor(_text)\n",
        "\n",
        "        # Set the final token in the sequence to `[CLS]`\n",
        "        data[-1, :] = self.classifier_token\n",
        "\n",
        "        #\n",
        "        return data, labels\n",
        "\n",
        "\n",
        "@option([NLPClassificationConfigs.n_classes,\n",
        "         NLPClassificationConfigs.vocab,\n",
        "         NLPClassificationConfigs.train_loader,\n",
        "         NLPClassificationConfigs.valid_loader])\n",
        "def ag_news(c: NLPClassificationConfigs):\n",
        "    \"\"\"\n",
        "    ### AG News dataset\n",
        "    This loads the AG News dataset and the set the values for\n",
        "     `n_classes', `vocab`, `train_loader`, and `valid_loader`.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get training and validation datasets\n",
        "    train, valid = torchtext.datasets.AG_NEWS(root=str(lab.get_data_path() / 'ag_news'), split=('train', 'test'))\n",
        "\n",
        "    # Load data to memory\n",
        "    with monit.section('Load data'):\n",
        "        from labml_nn.utils import MapStyleDataset\n",
        "\n",
        "        # Create [map-style datasets](../utils.html#map_style_dataset)\n",
        "        train, valid = MapStyleDataset(train), MapStyleDataset(valid)\n",
        "\n",
        "    # Get tokenizer\n",
        "    tokenizer = c.tokenizer\n",
        "\n",
        "    # Create a counter\n",
        "    counter = Counter()\n",
        "    # Collect tokens from training dataset\n",
        "    for (label, line) in train:\n",
        "        counter.update(tokenizer(line))\n",
        "    # Collect tokens from validation dataset\n",
        "    for (label, line) in valid:\n",
        "        counter.update(tokenizer(line))\n",
        "    # Create vocabulary\n",
        "    vocab = Vocab(counter, min_freq=1)\n",
        "\n",
        "    # Create training data loader\n",
        "    train_loader = DataLoader(train, batch_size=c.batch_size, shuffle=True,\n",
        "                              collate_fn=CollateFunc(tokenizer, vocab, c.seq_len, len(vocab), len(vocab) + 1))\n",
        "    # Create validation data loader\n",
        "    valid_loader = DataLoader(valid, batch_size=c.batch_size, shuffle=True,\n",
        "                              collate_fn=CollateFunc(tokenizer, vocab, c.seq_len, len(vocab), len(vocab) + 1))\n",
        "\n",
        "    # Return `n_classes', `vocab`, `train_loader`, and `valid_loader`\n",
        "    return 4, vocab, train_loader, valid_loader\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1JT5CrWIkO"
      },
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, encoder: Encoder, src_embed: Module, generator: nn.Linear):\n",
        "    super().__init__()\n",
        "    self.src_embed = src_embed\n",
        "    self.encoder = encoder\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.src_embed(x)\n",
        "    x = self.encoder(x, None)\n",
        "    x = self.generator(x[-1])\n",
        "    return x, None\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNXzvmJ2W-Ss"
      },
      "source": [
        "class Configs(NLPClassificationConfigs):\n",
        "  model: TransformerClassifier\n",
        "  transformer: TransformerConfigs\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo_D2nfSb-rD"
      },
      "source": [
        "@option(Configs.transformer)\n",
        "def _transformer_configs(c: Configs):\n",
        "    \"\"\"\n",
        "    ### Transformer configurations\n",
        "    \"\"\"\n",
        "\n",
        "    # We use our\n",
        "    # [configurable transformer implementation](../configs.html#TransformerConfigs)\n",
        "    conf = TransformerConfigs()\n",
        "    # Set the vocabulary sizes for embeddings and generating logits\n",
        "    conf.n_src_vocab = c.n_tokens\n",
        "    conf.n_tgt_vocab = c.n_tokens\n",
        "\n",
        "    #\n",
        "    return conf\n",
        "\n",
        "\n",
        "@option(TransformerConfigs.encoder_attn)\n",
        "def fnet_mix():\n",
        "    \"\"\"\n",
        "    Create `FNetMix` module that can replace the self-attention in\n",
        "    [transformer encoder layer](../models.html#TransformerLayer)\n",
        ".\n",
        "    \"\"\"\n",
        "    from labml_nn.transformers.fnet import FNetMix\n",
        "    return FNetMix()\n",
        "\n",
        "\n",
        "@option(Configs.model)\n",
        "def _model(c: Configs):\n",
        "    \"\"\"\n",
        "    Create classification model\n",
        "    \"\"\"\n",
        "    m = TransformerClassifier(c.transformer.encoder,\n",
        "                              c.transformer.src_embed,\n",
        "                              nn.Linear(c.d_model, c.n_classes)).to(c.device)\n",
        "\n",
        "    return m"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r0BGbShcH4F"
      },
      "source": [
        "\n",
        "def main():\n",
        "    # Create experiment\n",
        "    experiment.create(name=\"fnet\")\n",
        "    # Create configs\n",
        "    conf = Configs()\n",
        "    # Override configurations\n",
        "    experiment.configs(conf, {\n",
        "        # Use world level tokenizer\n",
        "        'tokenizer': 'basic_english',\n",
        "\n",
        "        # Train for $32$ epochs\n",
        "        'epochs': 32,\n",
        "        # Switch between training and validation for $10$ times\n",
        "        # per epoch\n",
        "        'inner_iterations': 10,\n",
        "\n",
        "        # Transformer configurations (same as defaults)\n",
        "        'transformer.d_model': 512,\n",
        "        'transformer.ffn.d_ff': 2048,\n",
        "        'transformer.n_heads': 8,\n",
        "        'transformer.n_layers': 6,\n",
        "\n",
        "        # Use [FNet](index.html) instead of self-a\n",
        "        # ttention\n",
        "        'transformer.encoder_attn': 'fnet_mix',\n",
        "\n",
        "        # Use [Noam optimizer](../../optimizers/noam.html)\n",
        "        'optimizer.optimizer': 'Noam',\n",
        "        'optimizer.learning_rate': 1.,\n",
        "    })\n",
        "\n",
        "    # Set models for saving and loading\n",
        "    experiment.add_pytorch_models({'model': conf.model})\n",
        "\n",
        "    # Start the experiment\n",
        "    with experiment.start():\n",
        "        # Run training\n",
        "        conf.run()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "shMCRhD3cLY_",
        "outputId": "e8ccc440-842d-495d-9ba0-26b645339008"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>Not a valid git repository: <strong>/home/micael</strong><span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span>\n",
              "\n",
              "Prepare model...\n",
              "  Prepare transformer...\n",
              "    Prepare n_tokens...\n",
              "      Prepare vocab...\n",
              "        Load data<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t391.29ms</span>\n",
              "        Prepare tokenizer<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t0.76ms</span>\n",
              "      Prepare vocab<span style=\"color: #E75C58\">...[FAIL]</span><span style=\"color: #208FFB\">\t2,763.99ms</span>\n",
              "    Prepare n_tokens<span style=\"color: #E75C58\">...[FAIL]</span><span style=\"color: #208FFB\">\t2,766.60ms</span>\n",
              "  Prepare transformer<span style=\"color: #E75C58\">...[FAIL]</span><span style=\"color: #208FFB\">\t2,769.02ms</span>\n",
              "Prepare model<span style=\"color: #E75C58\">...[FAIL]</span><span style=\"color: #208FFB\">\t2,770.76ms</span>\n",
              "</pre>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/home/micael/.local/lib/python3.9/site-packages/labml/internal/configs/base.py:322: Warning: Overriding option for encoder_attn: fnet_mix\n",
            "  calc_value = func(self)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-b84cc03e92c3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Set models for saving and loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pytorch_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Start the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__calculate\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Prepare {item}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mcalc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/config_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctionKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-feeae592d85b>\u001b[0m in \u001b[0;36m_model\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mCreate\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     m = TransformerClassifier(c.transformer.encoder,\n\u001b[0m\u001b[1;32m     35\u001b[0m                               \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                               nn.Linear(c.d_model, c.n_classes)).to(c.device)\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__calculate\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Prepare {item}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mcalc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/config_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctionKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-feeae592d85b>\u001b[0m in \u001b[0;36m_transformer_configs\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerConfigs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Set the vocabulary sizes for embeddings and generating logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_src_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tgt_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__calculate\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Prepare {item}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mcalc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/config_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctionKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b1faa53a96a1>\u001b[0m in \u001b[0;36m_n_tokens\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mGet\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \"\"\"\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calculate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/base.py\u001b[0m in \u001b[0;36m__calculate\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Prepare {item}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0mcalc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/labml/internal/configs/config_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctionKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b1faa53a96a1>\u001b[0m in \u001b[0;36mag_news\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# Create vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# Create training data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_freq'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQ5r158uDUa",
        "outputId": "fc2ad221-d140-4a23-a090-7d4de49db7de"
      },
      "source": [
        "def get_available_gpus():\n",
        "    return [ torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]\n",
        "get_available_gpus()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[_CudaDeviceProperties(name='GeForce RTX 3060', major=8, minor=6, total_memory=12053MB, multi_processor_count=28)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}